<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bayesian Optimization - AI4Bio Concepts</title>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        body {
            font-family: 'Lato', sans-serif;
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            line-height: 1.8;
            color: #333;
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
        }
        .navbar {
            background-color: #ffffff;
            border-bottom: 1px solid #e7e7e7;
            padding: 10px 0;
        }
        .navbar-nav {
            list-style-type: none;
            padding: 0;
            margin: 0;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .navbar-nav li {
            margin: 0 15px;
        }
        .navbar-nav a {
            color: #666666;
            text-decoration: none;
            font-weight: bold;
            font-size: 16px;
        }
        .home-link {
            color: #3498db !important;
        }
        .header {
            background: linear-gradient(135deg, #3498db 0%, #2980b9 100%);
            color: white;
            padding: 40px;
            border-radius: 15px;
            margin: 30px 0;
            text-align: center;
        }
        .header h1 {
            margin: 0 0 10px 0;
            font-size: 2.5em;
        }
        .header p {
            margin: 5px 0;
            font-size: 1.1em;
            opacity: 0.9;
        }
        .paper-ref {
            background: rgba(255,255,255,0.2);
            padding: 15px;
            border-radius: 10px;
            margin-top: 20px;
            font-size: 0.95em;
        }
        .content-section {
            background: white;
            border-radius: 15px;
            padding: 30px;
            margin: 25px 0;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        .content-section h2 {
            color: #3498db;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 20px;
        }
        .content-section h3 {
            color: #2980b9;
            margin-top: 25px;
        }
        .concept-box {
            background: #f8f9fa;
            border-left: 5px solid #3498db;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .concept-box.challenge {
            border-left-color: #e74c3c;
        }
        .concept-box.gp {
            border-left-color: #9b59b6;
        }
        .concept-box.acquisition {
            border-left-color: #27ae60;
        }
        .concept-box.practical {
            border-left-color: #f39c12;
        }
        .highlight-box {
            background: #d4edda;
            border: 1px solid #28a745;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
        }
        .highlight-box strong {
            color: #155724;
        }
        .warning-box {
            background: #fff3cd;
            border: 1px solid #ffc107;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
        }
        .warning-box strong {
            color: #856404;
        }
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        .comparison-table th,
        .comparison-table td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        .comparison-table th {
            background: #3498db;
            color: white;
        }
        .comparison-table tr:hover {
            background: #f5f5f5;
        }
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .stat-card {
            background: linear-gradient(135deg, #3498db 0%, #2980b9 100%);
            color: white;
            padding: 20px;
            border-radius: 10px;
            text-align: center;
        }
        .stat-card .number {
            font-size: 2em;
            font-weight: bold;
            margin-bottom: 5px;
        }
        .stat-card .label {
            font-size: 0.9em;
            opacity: 0.9;
        }
        .timeline {
            position: relative;
            padding: 20px 0;
            margin: 30px 0;
        }
        .timeline-item {
            position: relative;
            padding-left: 60px;
            margin-bottom: 30px;
        }
        .timeline-icon {
            position: absolute;
            left: 0;
            width: 40px;
            height: 40px;
            background: #3498db;
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
        }
        .timeline-content {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            border-left: 3px solid #3498db;
        }
        .method-card {
            background: white;
            border: 2px solid #3498db;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
        }
        .method-card h4 {
            color: #3498db;
            margin-top: 0;
        }
        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        .pros, .cons {
            padding: 15px;
            border-radius: 8px;
        }
        .pros {
            background: #d4edda;
            border-left: 4px solid #28a745;
        }
        .cons {
            background: #f8d7da;
            border-left: 4px solid #dc3545;
        }
        .decision-tree {
            background: #f8f9fa;
            padding: 25px;
            border-radius: 10px;
            margin: 20px 0;
        }
        .decision-node {
            background: white;
            border: 2px solid #3498db;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
            position: relative;
        }
        .decision-arrow {
            text-align: center;
            color: #3498db;
            font-size: 1.5em;
            margin: 10px 0;
        }
        .back-link {
            display: inline-block;
            margin: 20px 0;
            padding: 10px 20px;
            background: #3498db;
            color: white;
            text-decoration: none;
            border-radius: 8px;
            transition: all 0.3s ease;
        }
        .back-link:hover {
            background: #2980b9;
            transform: translateX(-5px);
        }
        .code-block {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 10px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.6;
            margin: 15px 0;
        }
        .code-block .comment {
            color: #75715e;
        }
        .code-block .keyword {
            color: #f92672;
        }
        .code-block .string {
            color: #e6db74;
        }
        .algorithm-box {
            background: #e8f4f8;
            border: 2px solid #3498db;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
        }
        .step {
            margin: 10px 0;
            padding-left: 20px;
        }
        .formula {
            background: white;
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            text-align: center;
            font-family: 'Times New Roman', serif;
            font-size: 1.1em;
        }
        @media (max-width: 768px) {
            .header h1 {
                font-size: 1.8em;
            }
            .pros-cons {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <nav class="navbar">
        <div class="container">
            <ul class="navbar-nav">
                <li><a href="../index.html" class="home-link"><i class="fas fa-book"></i> AI4Bio Concepts Handbook</a></li>
                <li><a href="../../"><i class="fas fa-home"></i> Xinru's Homepage</a></li>
            </ul>
        </div>
    </nav>

    <main role="main">
        <div class="container">
            <div class="header">
                <h1><i class="fas fa-chart-line"></i> Bayesian Optimization</h1>
                <p>Smart hyperparameter tuning for expensive black-box functions</p>
                <div class="paper-ref">
                    <strong>Key Application:</strong> Optimizing neural network hyperparameters with 10-100x fewer evaluations than grid search or random search
                </div>
            </div>

            <!-- The Problem -->
            <div class="content-section">
                <h2><i class="fas fa-exclamation-triangle"></i> The Problem</h2>
                
                <div class="concept-box challenge">
                    <h3>When Do You Need Bayesian Optimization?</h3>
                    <p><strong>Core Challenge:</strong> You need to optimize a function f(x) that is:</p>
                    <ul>
                        <li><strong>Expensive to evaluate</strong> - Each evaluation takes minutes to hours (training a deep learning model)</li>
                        <li><strong>Black-box</strong> - No gradient information available</li>
                        <li><strong>Noisy</strong> - Evaluations may have stochastic variability</li>
                        <li><strong>High-dimensional</strong> - Multiple parameters to tune simultaneously</li>
                    </ul>
                </div>

                <h3>Concrete Example: Neural Network Hyperparameter Tuning</h3>
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="number">Learning Rate</div>
                        <div class="label">0.0001 to 0.1 (continuous)</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">Batch Size</div>
                        <div class="label">16, 32, 64, 128 (discrete)</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">Dropout Rate</div>
                        <div class="label">0.0 to 0.5 (continuous)</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">Hidden Layers</div>
                        <div class="label">2, 3, 4, 5 (discrete)</div>
                    </div>
                </div>

                <div class="warning-box">
                    <strong><i class="fas fa-exclamation-circle"></i> The Cost Problem:</strong> 
                    If you have 4 hyperparameters with 10 values each, grid search requires 10<sup>4</sup> = 10,000 evaluations. If each evaluation takes 1 hour, that's 417 days of computation! Bayesian optimization can find near-optimal settings in 50-200 evaluations.
                </div>

                <h3>Why Traditional Methods Fail</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>Strategy</th>
                            <th>Evaluations Needed</th>
                            <th>Problem</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Grid Search</strong></td>
                            <td>Try all combinations</td>
                            <td>n<sup>d</sup> (exponential)</td>
                            <td>Curse of dimensionality</td>
                        </tr>
                        <tr>
                            <td><strong>Random Search</strong></td>
                            <td>Sample randomly</td>
                            <td>100-1000+</td>
                            <td>Ignores previous results</td>
                        </tr>
                        <tr>
                            <td><strong>Manual Tuning</strong></td>
                            <td>Human intuition</td>
                            <td>20-100</td>
                            <td>Slow, biased, not reproducible</td>
                        </tr>
                        <tr style="background: #d4edda;">
                            <td><strong>Bayesian Optimization</strong></td>
                            <td>Learn from evaluations</td>
                            <td><strong>50-200</strong></td>
                            <td><strong>Best balance of efficiency & performance</strong></td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- Core Concepts -->
            <div class="content-section">
                <h2><i class="fas fa-lightbulb"></i> Core Concepts</h2>
                
                <div class="concept-box">
                    <h3>The Big Picture</h3>
                    <p>Bayesian Optimization is a two-part iterative algorithm:</p>
                    <ol>
                        <li><strong>Surrogate Model (Gaussian Process):</strong> Build a probabilistic model of the objective function using past evaluations</li>
                        <li><strong>Acquisition Function:</strong> Use the surrogate to decide where to evaluate next, balancing exploration vs. exploitation</li>
                    </ol>
                </div>

                <h3>The Algorithm Loop</h3>
                <div class="algorithm-box">
                    <strong>Algorithm: Bayesian Optimization</strong><br><br>
                    <div class="step">1. Initialize with random evaluations: D = {(x₁, y₁), ..., (x_n, y_n)}</div>
                    <div class="step">2. <strong>Repeat</strong> until budget exhausted:</div>
                    <div class="step" style="padding-left: 40px;">a. Fit Gaussian Process to current data D</div>
                    <div class="step" style="padding-left: 40px;">b. Optimize acquisition function to find x_next</div>
                    <div class="step" style="padding-left: 40px;">c. Evaluate y_next = f(x_next)</div>
                    <div class="step" style="padding-left: 40px;">d. Update D = D ∪ {(x_next, y_next)}</div>
                    <div class="step">3. <strong>Return</strong> x* = argmax y in D</div>
                </div>

                <div class="timeline">
                    <div class="timeline-item">
                        <div class="timeline-icon">1</div>
                        <div class="timeline-content">
                            <h4>Random Initialization (5-10 evaluations)</h4>
                            <p>Sample initial points uniformly to get a rough sense of the landscape</p>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-icon">2</div>
                        <div class="timeline-content">
                            <h4>Build Surrogate Model</h4>
                            <p>Fit a Gaussian Process that predicts both mean and uncertainty for any point</p>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-icon">3</div>
                        <div class="timeline-content">
                            <h4>Acquisition Function Optimization</h4>
                            <p>Find the point that maximizes expected improvement (or another acquisition function)</p>
                        </div>
                    </div>
                    
                    <div class="timeline-item">
                        <div class="timeline-icon">4</div>
                        <div class="timeline-content">
                            <h4>Evaluate & Update</h4>
                            <p>Train model at selected point, add result to dataset, repeat until convergence</p>
                        </div>
                    </div>
                </div>
            </div>

            <!-- Gaussian Processes -->
            <div class="content-section">
                <h2><i class="fas fa-wave-square"></i> Component 1: Gaussian Processes</h2>
                
                <div class="concept-box gp">
                    <h3>What is a Gaussian Process?</h3>
                    <p>A Gaussian Process (GP) is a distribution over functions. Think of it as "infinite-dimensional Gaussian distribution" where any finite set of points follows a multivariate Gaussian.</p>
                </div>

                <h3>Key Properties</h3>
                <div class="highlight-box">
                    <strong><i class="fas fa-check-circle"></i> Why GPs are Perfect for Bayesian Optimization:</strong>
                    <ul>
                        <li><strong>Predictive mean:</strong> Best guess of function value at any point</li>
                        <li><strong>Predictive variance:</strong> Uncertainty estimate (confidence intervals)</li>
                        <li><strong>Interpolation:</strong> Passes through observed data exactly (zero uncertainty at training points)</li>
                        <li><strong>Smooth:</strong> Assumes nearby points have similar values (controlled by kernel)</li>
                    </ul>
                </div>

                <h3>Mathematical Formulation</h3>
                <div class="formula">
                    f(x) ~ GP(m(x), k(x, x'))<br><br>
                    <strong>Mean function:</strong> m(x) = E[f(x)]<br>
                    <strong>Kernel function:</strong> k(x, x') = Cov[f(x), f(x')]
                </div>

                <h3>Common Kernels</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Kernel</th>
                            <th>Formula</th>
                            <th>Use Case</th>
                            <th>Smoothness</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>RBF (Squared Exponential)</strong></td>
                            <td>k(x,x') = σ² exp(-||x-x'||²/2ℓ²)</td>
                            <td>Default choice, smooth functions</td>
                            <td>Infinitely differentiable</td>
                        </tr>
                        <tr>
                            <td><strong>Matérn 5/2</strong></td>
                            <td>k(x,x') = σ²(1+√5r+5r²/3)exp(-√5r)</td>
                            <td>Realistic, less smooth than RBF</td>
                            <td>Twice differentiable</td>
                        </tr>
                        <tr>
                            <td><strong>Matérn 3/2</strong></td>
                            <td>k(x,x') = σ²(1+√3r)exp(-√3r)</td>
                            <td>Rougher functions</td>
                            <td>Once differentiable</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Hyperparameters of the GP</h3>
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="number">σ²</div>
                        <div class="label">Signal variance (output scale)</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">ℓ</div>
                        <div class="label">Length scale (how far correlation extends)</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">σ_n²</div>
                        <div class="label">Noise variance (observation noise)</div>
                    </div>
                </div>

                <div class="warning-box">
                    <strong><i class="fas fa-info-circle"></i> Learning GP Hyperparameters:</strong>
                    These are typically learned by maximizing the marginal likelihood (evidence) of the observed data. This happens automatically in modern libraries like GPyTorch or scikit-learn.
                </div>

                <h3>GP Predictions</h3>
                <div class="concept-box">
                    <p>Given observations D = {(x₁, y₁), ..., (x_n, y_n)}, the GP posterior at a new point x* is:</p>
                    <div class="formula">
                        <strong>Posterior mean:</strong> μ(x*) = k(x*, X)(K + σ_n²I)⁻¹y<br>
                        <strong>Posterior variance:</strong> σ²(x*) = k(x*, x*) - k(x*, X)(K + σ_n²I)⁻¹k(X, x*)
                    </div>
                    <p>Where K is the kernel matrix K_ij = k(x_i, x_j)</p>
                </div>

                <h3>Implementation Example</h3>
                <div class="code-block">
<span class="keyword">from</span> sklearn.gaussian_process <span class="keyword">import</span> GaussianProcessRegressor
<span class="keyword">from</span> sklearn.gaussian_process.kernels <span class="keyword">import</span> Matern, ConstantKernel
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># Define kernel: signal variance * Matérn 5/2</span>
kernel = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)

<span class="comment"># Create GP</span>
gp = GaussianProcessRegressor(
    kernel=kernel,
    alpha=1e-6,  <span class="comment"># noise variance</span>
    n_restarts_optimizer=10
)

<span class="comment"># Fit to observed data</span>
X_observed = np.array([[0.1], [0.5], [0.9]])
y_observed = np.array([0.3, 0.8, 0.2])
gp.fit(X_observed, y_observed)

<span class="comment"># Predict at new points (with uncertainty)</span>
X_test = np.linspace(0, 1, 100).reshape(-1, 1)
y_mean, y_std = gp.predict(X_test, return_std=<span class="keyword">True</span>)

<span class="comment"># Plot with confidence intervals</span>
plt.plot(X_test, y_mean, <span class="string">'b-'</span>, label=<span class="string">'GP mean'</span>)
plt.fill_between(X_test.ravel(), 
                 y_mean - 2*y_std, 
                 y_mean + 2*y_std,
                 alpha=0.2, label=<span class="string">'95% confidence'</span>)
plt.scatter(X_observed, y_observed, c=<span class="string">'r'</span>, label=<span class="string">'Observations'</span>)
                </div>
            </div>

            <!-- Acquisition Functions -->
            <div class="content-section">
                <h2><i class="fas fa-bullseye"></i> Component 2: Acquisition Functions</h2>
                
                <div class="concept-box acquisition">
                    <h3>The Exploration-Exploitation Trade-off</h3>
                    <p><strong>Exploitation:</strong> Sample where the GP predicts high values (exploit what you know)</p>
                    <p><strong>Exploration:</strong> Sample where the GP has high uncertainty (explore the unknown)</p>
                    <p><strong>Goal:</strong> Balance both to efficiently find the global optimum</p>
                </div>

                <h3>Expected Improvement (EI)</h3>
                <div class="method-card">
                    <h4>Most Popular Acquisition Function</h4>
                    <div class="formula">
                        EI(x) = E[max(f(x) - f(x⁺), 0)]<br><br>
                        = (μ(x) - f(x⁺) - ξ)Φ(Z) + σ(x)φ(Z)<br><br>
                        where Z = (μ(x) - f(x⁺) - ξ) / σ(x)
                    </div>
                    <p><strong>f(x⁺):</strong> Best observed value so far (incumbent)</p>
                    <p><strong>ξ:</strong> Exploration parameter (typically 0.01)</p>
                    <p><strong>Φ, φ:</strong> CDF and PDF of standard normal</p>
                </div>

                <div class="pros-cons">
                    <div class="pros">
                        <h4><i class="fas fa-check-circle"></i> Pros of EI</h4>
                        <ul>
                            <li>Closed-form solution (fast to compute)</li>
                            <li>Natural exploration-exploitation balance</li>
                            <li>Zero at observed points (encourages exploration)</li>
                            <li>Most widely used and well-tested</li>
                        </ul>
                    </div>
                    <div class="cons">
                        <h4><i class="fas fa-times-circle"></i> Cons of EI</h4>
                        <ul>
                            <li>Can be overly exploitative</li>
                            <li>May get stuck in local optima</li>
                            <li>Sensitive to ξ parameter</li>
                        </ul>
                    </div>
                </div>

                <h3>Other Acquisition Functions</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Function</th>
                            <th>Formula</th>
                            <th>Behavior</th>
                            <th>Use Case</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Upper Confidence Bound (UCB)</strong></td>
                            <td>UCB(x) = μ(x) + κσ(x)</td>
                            <td>More exploration</td>
                            <td>When you want guaranteed convergence</td>
                        </tr>
                        <tr>
                            <td><strong>Probability of Improvement (PI)</strong></td>
                            <td>PI(x) = Φ((μ(x) - f(x⁺))/σ(x))</td>
                            <td>More exploitation</td>
                            <td>When near-optimal is good enough</td>
                        </tr>
                        <tr>
                            <td><strong>Knowledge Gradient (KG)</strong></td>
                            <td>Complex (1-step lookahead)</td>
                            <td>Myopic optimal</td>
                            <td>Finite budget optimization</td>
                        </tr>
                        <tr>
                            <td><strong>Entropy Search</strong></td>
                            <td>Mutual information based</td>
                            <td>Information-theoretic</td>
                            <td>When you care about the optimum location</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Tuning Exploration vs. Exploitation</h3>
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="number">κ = 0.1-1.0</div>
                        <div class="label">UCB: Lower = exploit, Higher = explore</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">ξ = 0.01</div>
                        <div class="label">EI: Typical value balances both</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">Adaptive</div>
                        <div class="label">Decrease exploration over time</div>
                    </div>
                </div>

                <h3>Implementation Example</h3>
                <div class="code-block">
<span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm

<span class="keyword">def</span> <span class="function">expected_improvement</span>(X, gp, best_y, xi=0.01):
    <span class="string">"""
    Compute Expected Improvement acquisition function.
    
    Args:
        X: Points to evaluate (n_points, n_features)
        gp: Fitted Gaussian Process
        best_y: Best observed value so far
        xi: Exploration parameter
    
    Returns:
        EI values for each point in X
    """</span>
    mu, sigma = gp.predict(X, return_std=<span class="keyword">True</span>)
    sigma = sigma.reshape(-1, 1)
    
    <span class="comment"># Calculate Z score</span>
    Z = (mu - best_y - xi) / sigma
    
    <span class="comment"># Expected Improvement formula</span>
    ei = (mu - best_y - xi) * norm.cdf(Z) + sigma * norm.pdf(Z)
    ei[sigma == 0.0] = 0.0  <span class="comment"># Handle numerical issues</span>
    
    <span class="keyword">return</span> ei

<span class="keyword">def</span> <span class="function">propose_next_point</span>(gp, bounds, best_y, n_restarts=25):
    <span class="string">"""Find point that maximizes acquisition function."""</span>
    <span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> minimize
    
    best_x = <span class="keyword">None</span>
    best_acquisition = -<span class="keyword">float</span>(<span class="string">'inf'</span>)
    
    <span class="comment"># Multi-start optimization</span>
    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_restarts):
        x0 = np.random.uniform(bounds[:, 0], bounds[:, 1])
        
        res = minimize(
            <span class="keyword">lambda</span> x: -expected_improvement(x.reshape(1, -1), gp, best_y),
            x0,
            bounds=bounds,
            method=<span class="string">'L-BFGS-B'</span>
        )
        
        <span class="keyword">if</span> -res.fun > best_acquisition:
            best_acquisition = -res.fun
            best_x = res.x
    
    <span class="keyword">return</span> best_x
                </div>
            </div>

            <!-- Complete Implementation -->
            <div class="content-section">
                <h2><i class="fas fa-code"></i> Complete Implementation</h2>
                
                <h3>Full Bayesian Optimization Loop</h3>
                <div class="code-block">
<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">from</span> sklearn.gaussian_process <span class="keyword">import</span> GaussianProcessRegressor
<span class="keyword">from</span> sklearn.gaussian_process.kernels <span class="keyword">import</span> Matern

<span class="keyword">def</span> <span class="function">bayesian_optimization</span>(
    objective_func,
    bounds,
    n_initial=5,
    n_iterations=20,
    acquisition=<span class="string">'ei'</span>,
    random_state=42
):
    <span class="string">"""
    Bayesian Optimization with Gaussian Process surrogate.
    
    Args:
        objective_func: Function to maximize (takes array, returns scalar)
        bounds: Array of shape (n_dims, 2) with [min, max] for each dimension
        n_initial: Number of random initialization points
        n_iterations: Number of optimization iterations
        acquisition: 'ei' (Expected Improvement) or 'ucb' (Upper Confidence Bound)
        random_state: Random seed for reproducibility
    
    Returns:
        best_x: Best point found
        best_y: Best value found
        X_all: All evaluated points
        y_all: All observed values
    """</span>
    np.random.seed(random_state)
    n_dims = bounds.shape[0]
    
    <span class="comment"># Step 1: Random initialization</span>
    X_init = np.random.uniform(
        bounds[:, 0], 
        bounds[:, 1], 
        size=(n_initial, n_dims)
    )
    y_init = np.array([objective_func(x) <span class="keyword">for</span> x <span class="keyword">in</span> X_init])
    
    X_all = X_init.copy()
    y_all = y_init.copy()
    
    <span class="comment"># Initialize GP</span>
    kernel = Matern(nu=2.5)
    gp = GaussianProcessRegressor(
        kernel=kernel,
        alpha=1e-6,
        normalize_y=<span class="keyword">True</span>,
        n_restarts_optimizer=5
    )
    
    <span class="comment"># Step 2: Bayesian optimization loop</span>
    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):
        <span class="comment"># Fit GP to current data</span>
        gp.fit(X_all, y_all)
        
        <span class="comment"># Find next point to evaluate</span>
        best_y = np.max(y_all)
        x_next = propose_next_point(gp, bounds, best_y)
        
        <span class="comment"># Evaluate objective</span>
        y_next = objective_func(x_next)
        
        <span class="comment"># Update dataset</span>
        X_all = np.vstack([X_all, x_next])
        y_all = np.append(y_all, y_next)
        
        <span class="keyword">print</span>(<span class="string">f"Iteration {iteration+1}/{n_iterations}: "</span>
              <span class="string">f"y_next = {y_next:.4f}, best_y = {np.max(y_all):.4f}"</span>)
    
    <span class="comment"># Return best result</span>
    best_idx = np.argmax(y_all)
    best_x = X_all[best_idx]
    best_y = y_all[best_idx]
    
    <span class="keyword">return</span> best_x, best_y, X_all, y_all

<span class="comment"># Example usage: Optimize a test function</span>
<span class="keyword">def</span> <span class="function">branin</span>(x):
    <span class="string">"""Branin function (common benchmark with 3 global optima)."""</span>
    x1, x2 = x[0], x[1]
    a, b, c = 1, 5.1/(4*np.pi**2), 5/np.pi
    r, s, t = 6, 10, 1/(8*np.pi)
    <span class="keyword">return</span> -(a*(x2 - b*x1**2 + c*x1 - r)**2 + s*(1-t)*np.cos(x1) + s)

bounds = np.array([[-5, 10], [0, 15]])
best_x, best_y, X_all, y_all = bayesian_optimization(
    branin, 
    bounds, 
    n_initial=10, 
    n_iterations=30
)

<span class="keyword">print</span>(<span class="string">f"\nBest point: {best_x}"</span>)
<span class="keyword">print</span>(<span class="string">f"Best value: {best_y:.4f}"</span>)
                </div>
            </div>

            <!-- Applications in Biology -->
            <div class="content-section">
                <h2><i class="fas fa-dna"></i> Applications in Computational Biology</h2>
                
                <div class="method-card">
                    <h4>1. Neural Network Hyperparameter Tuning</h4>
                    <p><strong>Task:</strong> Find optimal hyperparameters for scVI, scGPT, or other single-cell models</p>
                    <p><strong>Parameters to optimize:</strong></p>
                    <ul>
                        <li>Learning rate (log scale: 1e-5 to 1e-2)</li>
                        <li>Latent dimension (10 to 100)</li>
                        <li>Number of hidden layers (1 to 5)</li>
                        <li>Dropout rate (0.0 to 0.5)</li>
                        <li>Batch size (32, 64, 128, 256)</li>
                    </ul>
                    <p><strong>Objective:</strong> Maximize validation set accuracy or minimize reconstruction loss</p>
                    <p><strong>Typical results:</strong> 50 evaluations vs. 500+ for random search</p>
                </div>

                <div class="method-card">
                    <h4>2. Protein Structure Prediction Refinement</h4>
                    <p><strong>Task:</strong> Optimize AlphaFold2 or ESMFold parameters for a specific protein family</p>
                    <p><strong>Parameters to optimize:</strong></p>
                    <ul>
                        <li>MSA depth (number of sequences)</li>
                        <li>Template selection threshold</li>
                        <li>Recycle iterations (1 to 20)</li>
                        <li>Distillation loss weight</li>
                    </ul>
                    <p><strong>Objective:</strong> Maximize pLDDT score or experimental validation metric (RMSD)</p>
                </div>

                <div class="method-card">
                    <h4>3. Drug Discovery: Molecular Property Optimization</h4>
                    <p><strong>Task:</strong> Find molecules with desired properties using generative models</p>
                    <p><strong>Parameters to optimize:</strong></p>
                    <ul>
                        <li>VAE latent codes (50-200 dimensions)</li>
                        <li>Temperature for sampling</li>
                        <li>Beam search width</li>
                    </ul>
                    <p><strong>Objective:</strong> Maximize drug-likeness score (QED) + target binding affinity</p>
                    <p><strong>Constraint:</strong> Each evaluation requires expensive molecular dynamics simulation</p>
                </div>

                <div class="method-card">
                    <h4>4. Experimental Design: Optimal Dosing</h4>
                    <p><strong>Task:</strong> Find optimal drug concentration and timing for in vitro experiments</p>
                    <p><strong>Parameters to optimize:</strong></p>
                    <ul>
                        <li>Drug concentration (0.1 nM to 100 μM)</li>
                        <li>Treatment duration (1h to 72h)</li>
                        <li>Temperature (20°C to 40°C)</li>
                    </ul>
                    <p><strong>Objective:</strong> Maximize target protein expression or cell viability</p>
                    <p><strong>Key advantage:</strong> Minimize number of expensive lab experiments</p>
                </div>

                <h3>Real-World Case Study: scVI Hyperparameter Optimization</h3>
                <div class="concept-box">
                    <p><strong>Dataset:</strong> 50,000 single-cell RNA-seq profiles, 20 cell types</p>
                    <p><strong>Model:</strong> scVI (Variational Autoencoder)</p>
                    <p><strong>Search space:</strong></p>
                    <ul>
                        <li>Learning rate: [1e-5, 1e-2] (log scale)</li>
                        <li>Latent dimensions: [10, 50]</li>
                        <li>Number of hidden layers: [1, 3]</li>
                        <li>Dropout: [0.0, 0.5]</li>
                    </ul>
                    <p><strong>Results:</strong></p>
                    <ul>
                        <li>Random search (200 evals): Best accuracy = 87.3%</li>
                        <li>Bayesian optimization (50 evals): Best accuracy = 91.2%</li>
                        <li><strong>4x fewer evaluations, 4.5% better performance</strong></li>
                    </ul>
                </div>
            </div>

            <!-- Practical Considerations -->
            <div class="content-section">
                <h2><i class="fas fa-wrench"></i> Practical Considerations</h2>
                
                <h3>When to Use Bayesian Optimization</h3>
                <div class="decision-tree">
                    <div class="decision-node">
                        <strong>Q: How expensive is each evaluation?</strong>
                    </div>
                    <div class="decision-arrow">↓</div>
                    
                    <div class="decision-node">
                        <strong>Cheap (< 1 second)</strong>
                        <ul>
                            <li>❌ Use grid search or random search</li>
                            <li>Bayesian optimization overhead not worth it</li>
                        </ul>
                    </div>
                    
                    <div class="decision-node">
                        <strong>Moderate (1 second to 1 minute)</strong>
                        <ul>
                            <li>⚠️ Consider Bayesian optimization if budget < 100 evaluations</li>
                            <li>Otherwise random search is fine</li>
                        </ul>
                    </div>
                    
                    <div class="decision-node">
                        <strong>Expensive (> 1 minute)</strong>
                        <ul>
                            <li>✅ Use Bayesian optimization</li>
                            <li>Especially if budget < 50-200 evaluations</li>
                            <li>The more expensive, the more beneficial</li>
                        </ul>
                    </div>
                </div>

                <h3>Handling Different Parameter Types</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Parameter Type</th>
                            <th>Encoding</th>
                            <th>Example</th>
                            <th>Notes</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Continuous</strong></td>
                            <td>Use as-is</td>
                            <td>Dropout rate: [0.0, 0.5]</td>
                            <td>Most natural for GP</td>
                        </tr>
                        <tr>
                            <td><strong>Log-scale continuous</strong></td>
                            <td>Optimize in log space</td>
                            <td>Learning rate: log[1e-5, 1e-2]</td>
                            <td>Better for parameters spanning orders of magnitude</td>
                        </tr>
                        <tr>
                            <td><strong>Integer</strong></td>
                            <td>Round continuous values</td>
                            <td>Latent dim: round([10, 100])</td>
                            <td>May lose some efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>Categorical</strong></td>
                            <td>One-hot or ordinal encoding</td>
                            <td>Optimizer: {Adam, SGD, RMSprop}</td>
                            <td>Use specialized kernels (mixed kernels)</td>
                        </tr>
                        <tr>
                            <td><strong>Conditional</strong></td>
                            <td>Tree-structured Parzen Estimator</td>
                            <td>If optimizer=Adam, tune β₁, β₂</td>
                            <td>Switch to TPE or SMAC algorithms</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Common Pitfalls and Solutions</h3>
                <div class="concept-box practical">
                    <h4>Pitfall 1: Not enough initial random samples</h4>
                    <p><strong>Problem:</strong> GP has no data to fit initially</p>
                    <p><strong>Solution:</strong> Use 5-10 random points per dimension (e.g., 20 samples for 4D problem)</p>
                </div>

                <div class="concept-box practical">
                    <h4>Pitfall 2: Noisy objective function</h4>
                    <p><strong>Problem:</strong> Evaluation results vary (e.g., different random seeds in training)</p>
                    <p><strong>Solution:</strong> Increase GP noise parameter (alpha) or average multiple evaluations</p>
                </div>

                <div class="concept-box practical">
                    <h4>Pitfall 3: Too many dimensions (curse of dimensionality)</h4>
                    <p><strong>Problem:</strong> GP degrades with >20 dimensions</p>
                    <p><strong>Solution:</strong> Use random forests (SMAC algorithm) or neural networks (DNGO) as surrogates</p>
                </div>

                <div class="concept-box practical">
                    <h4>Pitfall 4: Constraints or multi-objective optimization</h4>
                    <p><strong>Problem:</strong> Some configurations are invalid (e.g., batch size > dataset size)</p>
                    <p><strong>Solution:</strong> Use constrained BO variants or scalarize multiple objectives</p>
                </div>

                <h3>Scaling to High Dimensions</h3>
                <div class="warning-box">
                    <strong><i class="fas fa-exclamation-triangle"></i> Dimensionality Limits:</strong>
                    Standard GP-based BO works well up to ~10-15 dimensions. Beyond that, consider:
                    <ul>
                        <li><strong>Random embeddings:</strong> Project to lower-dimensional space (REMBO)</li>
                        <li><strong>Additive kernels:</strong> Assume function decomposes as sum of low-dimensional functions</li>
                        <li><strong>Alternative surrogates:</strong> Random forests (SMAC), neural networks (DNGO), or TPE</li>
                        <li><strong>Evolutionary strategies:</strong> CMA-ES for dimensions > 50</li>
                    </ul>
                </div>
            </div>

            <!-- Software Libraries -->
            <div class="content-section">
                <h2><i class="fas fa-laptop-code"></i> Software Libraries</h2>
                
                <h3>Popular Python Libraries</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Library</th>
                            <th>Focus</th>
                            <th>Best For</th>
                            <th>Pros</th>
                            <th>Cons</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>scikit-optimize</strong></td>
                            <td>Simple BO</td>
                            <td>Getting started, prototyping</td>
                            <td>Easy to use, lightweight</td>
                            <td>Basic features only</td>
                        </tr>
                        <tr>
                            <td><strong>GPyOpt</strong></td>
                            <td>GP optimization</td>
                            <td>Research, custom kernels</td>
                            <td>Flexible, well-documented</td>
                            <td>No longer maintained</td>
                        </tr>
                        <tr>
                            <td><strong>BoTorch</strong></td>
                            <td>Modern BO</td>
                            <td>State-of-the-art methods</td>
                            <td>GPU support, scalable, active development</td>
                            <td>Steeper learning curve</td>
                        </tr>
                        <tr>
                            <td><strong>Optuna</strong></td>
                            <td>Hyperparameter tuning</td>
                            <td>ML pipelines, production</td>
                            <td>Easy integration, TPE algorithm, pruning</td>
                            <td>Not pure GP-based</td>
                        </tr>
                        <tr>
                            <td><strong>Hyperopt</strong></td>
                            <td>TPE algorithm</td>
                            <td>High-dimensional spaces</td>
                            <td>Handles conditionals well</td>
                            <td>Older API design</td>
                        </tr>
                        <tr>
                            <td><strong>Ax Platform</strong></td>
                            <td>Enterprise BO</td>
                            <td>Large-scale experiments</td>
                            <td>Multi-objective, parallel, databases</td>
                            <td>Complex setup</td>
                        </tr>
                    </tbody>
                </table>

                <h3>Quick Start Examples</h3>
                
                <h4>1. scikit-optimize (Simplest)</h4>
                <div class="code-block">
<span class="keyword">from</span> skopt <span class="keyword">import</span> gp_minimize
<span class="keyword">from</span> skopt.space <span class="keyword">import</span> Real, Integer

<span class="comment"># Define search space</span>
space = [
    Real(1e-6, 1e-1, prior=<span class="string">'log-uniform'</span>, name=<span class="string">'learning_rate'</span>),
    Integer(10, 100, name=<span class="string">'latent_dim'</span>),
    Real(0.0, 0.5, name=<span class="string">'dropout'</span>)
]

<span class="comment"># Define objective (negative because we minimize)</span>
<span class="keyword">def</span> <span class="function">objective</span>(params):
    lr, latent_dim, dropout = params
    <span class="comment"># Train model and get validation accuracy</span>
    accuracy = train_model(lr, latent_dim, dropout)
    <span class="keyword">return</span> -accuracy  <span class="comment"># Negative because we minimize</span>

<span class="comment"># Run optimization</span>
result = gp_minimize(
    objective,
    space,
    n_calls=50,
    n_initial_points=10,
    acq_func=<span class="string">'EI'</span>,
    random_state=42
)

<span class="keyword">print</span>(<span class="string">f"Best params: {result.x}"</span>)
<span class="keyword">print</span>(<span class="string">f"Best score: {-result.fun}"</span>)
                </div>

                <h4>2. Optuna (Most Popular for ML)</h4>
                <div class="code-block">
<span class="keyword">import</span> optuna

<span class="keyword">def</span> <span class="function">objective</span>(trial):
    <span class="comment"># Sample hyperparameters</span>
    lr = trial.suggest_float(<span class="string">'learning_rate'</span>, 1e-6, 1e-1, log=<span class="keyword">True</span>)
    latent_dim = trial.suggest_int(<span class="string">'latent_dim'</span>, 10, 100)
    dropout = trial.suggest_float(<span class="string">'dropout'</span>, 0.0, 0.5)
    n_layers = trial.suggest_int(<span class="string">'n_layers'</span>, 1, 5)
    
    <span class="comment"># Train model</span>
    accuracy = train_model(lr, latent_dim, dropout, n_layers)
    
    <span class="keyword">return</span> accuracy  <span class="comment"># Optuna maximizes by default</span>

<span class="comment"># Create study and optimize</span>
study = optuna.create_study(
    direction=<span class="string">'maximize'</span>,
    sampler=optuna.samplers.TPESampler()
)
study.optimize(objective, n_trials=50)

<span class="keyword">print</span>(<span class="string">f"Best params: {study.best_params}"</span>)
<span class="keyword">print</span>(<span class="string">f"Best score: {study.best_value}"</span>)

<span class="comment"># Visualize optimization history</span>
optuna.visualization.plot_optimization_history(study)
optuna.visualization.plot_param_importances(study)
                </div>

                <h4>3. BoTorch (Most Advanced)</h4>
                <div class="code-block">
<span class="keyword">import</span> torch
<span class="keyword">from</span> botorch.models <span class="keyword">import</span> SingleTaskGP
<span class="keyword">from</span> botorch.fit <span class="keyword">import</span> fit_gpytorch_model
<span class="keyword">from</span> botorch.acquisition <span class="keyword">import</span> ExpectedImprovement
<span class="keyword">from</span> botorch.optim <span class="keyword">import</span> optimize_acqf
<span class="keyword">from</span> gpytorch.mlls <span class="keyword">import</span> ExactMarginalLogLikelihood

<span class="comment"># Initialize with random samples</span>
train_X = torch.rand(10, 3)  <span class="comment"># 10 points, 3D</span>
train_Y = torch.tensor([objective(x) <span class="keyword">for</span> x <span class="keyword">in</span> train_X]).unsqueeze(-1)

<span class="comment"># Optimization loop</span>
<span class="keyword">for</span> iteration <span class="keyword">in</span> range(40):
    <span class="comment"># Fit GP</span>
    gp = SingleTaskGP(train_X, train_Y)
    mll = ExactMarginalLogLikelihood(gp.likelihood, gp)
    fit_gpytorch_model(mll)
    
    <span class="comment"># Optimize acquisition function</span>
    EI = ExpectedImprovement(gp, best_f=train_Y.max())
    candidate, _ = optimize_acqf(
        EI,
        bounds=torch.stack([torch.zeros(3), torch.ones(3)]),
        q=1,
        num_restarts=10,
        raw_samples=100
    )
    
    <span class="comment"># Evaluate and update</span>
    new_Y = objective(candidate)
    train_X = torch.cat([train_X, candidate])
    train_Y = torch.cat([train_Y, new_Y.unsqueeze(-1)])
    
    <span class="keyword">print</span>(<span class="string">f"Iteration {iteration+1}: Best = {train_Y.max().item():.4f}"</span>)
                </div>
            </div>

            <!-- Decision Guide -->
            <div class="content-section">
                <h2><i class="fas fa-map-signs"></i> Algorithm Selection Guide</h2>
                
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Your Situation</th>
                            <th>Recommended Tool</th>
                            <th>Alternative</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Quick hyperparameter tuning for sklearn/PyTorch</td>
                            <td><strong>Optuna</strong></td>
                            <td>scikit-optimize</td>
                        </tr>
                        <tr>
                            <td>Need state-of-the-art acquisition functions</td>
                            <td><strong>BoTorch</strong></td>
                            <td>Ax Platform</td>
                        </tr>
                        <tr>
                            <td>High-dimensional (>20D) or categorical parameters</td>
                            <td><strong>Optuna (TPE)</strong></td>
                            <td>Hyperopt</td>
                        </tr>
                        <tr>
                            <td>Multi-objective optimization</td>
                            <td><strong>Ax Platform</strong></td>
                            <td>BoTorch</td>
                        </tr>
                        <tr>
                            <td>Parallel evaluations on cluster</td>
                            <td><strong>Ax Platform</strong></td>
                            <td>BoTorch with batch acquisition</td>
                        </tr>
                        <tr>
                            <td>Learning/teaching/prototyping</td>
                            <td><strong>scikit-optimize</strong></td>
                            <td>GPyOpt</td>
                        </tr>
                        <tr>
                            <td>GPU acceleration needed</td>
                            <td><strong>BoTorch</strong></td>
                            <td>GPyTorch directly</td>
                        </tr>
                        <tr>
                            <td>Expensive simulations (physics, chemistry)</td>
                            <td><strong>BoTorch</strong></td>
                            <td>Ax Platform</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <!-- Advanced Topics -->
            <div class="content-section">
                <h2><i class="fas fa-graduation-cap"></i> Advanced Topics</h2>
                
                <h3>1. Batch Bayesian Optimization</h3>
                <div class="concept-box">
                    <p><strong>Problem:</strong> Standard BO is sequential. What if you can evaluate multiple points in parallel?</p>
                    <p><strong>Solutions:</strong></p>
                    <ul>
                        <li><strong>q-Expected Improvement:</strong> Joint acquisition for q points simultaneously</li>
                        <li><strong>Local penalization:</strong> Penalize acquisition near recently selected points</li>
                        <li><strong>Thompson sampling:</strong> Sample from GP posterior and optimize each sample</li>
                    </ul>
                    <p><strong>Use case:</strong> Distributed hyperparameter search on GPU cluster</p>
                </div>

                <h3>2. Multi-Fidelity Optimization</h3>
                <div class="concept-box">
                    <p><strong>Idea:</strong> Approximate evaluations are cheaper (e.g., train for 1 epoch vs. 100 epochs)</p>
                    <p><strong>Method:</strong> Use multi-fidelity GP that models correlation between fidelities</p>
                    <p><strong>Benefit:</strong> Spend budget on cheap evaluations to guide expensive ones</p>
                    <p><strong>Implementation:</strong> BOHB (Bayesian Optimization + HyperBand)</p>
                </div>

                <h3>3. Constrained Bayesian Optimization</h3>
                <div class="concept-box">
                    <p><strong>Problem:</strong> Some configurations violate constraints (e.g., memory budget, runtime limit)</p>
                    <p><strong>Solution:</strong> Model constraints with separate GPs, use constrained acquisition</p>
                    <div class="formula">
                        Constrained EI(x) = EI(x) × P(all constraints satisfied)
                    </div>
                </div>

                <h3>4. Multi-Objective Bayesian Optimization</h3>
                <div class="concept-box">
                    <p><strong>Problem:</strong> Optimize multiple competing objectives (e.g., accuracy vs. inference time)</p>
                    <p><strong>Solution:</strong> Find Pareto frontier using multi-objective acquisition functions</p>
                    <p><strong>Acquisition:</strong> Expected Hypervolume Improvement (EHVI)
