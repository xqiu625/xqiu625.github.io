<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Two-Stage Foundation Model Training - AI4Bio Learning Hub</title>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        body {
            font-family: 'Lato', sans-serif;
            background: linear-gradient(135deg, #f5f7fa 0%, #e8d5d9 100%);
            line-height: 1.6;
            color: #333;
            margin: 0;
            padding: 0;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        .navbar {
            background-color: #ffffff;
            border-bottom: 1px solid #e7e7e7;
            padding: 10px 0;
        }
        .navbar-nav {
            list-style-type: none;
            padding: 0;
            margin: 0;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .navbar-nav li {
            margin: 0 15px;
        }
        .navbar-nav a {
            color: #666666;
            text-decoration: none;
            font-weight: bold;
            font-size: 16px;
        }
        .navbar-nav a:hover {
            color: #B01C3C;
        }
        .home-link {
            color: #B01C3C !important;
        }
        .header {
            background: white;
            border-radius: 15px;
            padding: 40px;
            margin: 30px 0;
            box-shadow: 0 4px 15px rgba(125, 20, 57, 0.15);
            text-align: center;
        }
        .header h1 {
            color: #7D1439;
            font-size: 2.5em;
            margin-bottom: 15px;
        }
        .header .subtitle {
            font-size: 1.2em;
            color: #666;
            margin-bottom: 20px;
        }
        .header .paper-ref {
            background: rgba(125, 20, 57, 0.1);
            padding: 15px;
            border-radius: 10px;
            margin-top: 20px;
            font-size: 0.95em;
            border-left: 4px solid #B01C3C;
        }
        .content-section {
            background: white;
            border-radius: 15px;
            padding: 40px;
            margin: 30px 0;
            box-shadow: 0 4px 15px rgba(125, 20, 57, 0.15);
        }
        .content-section h2 {
            color: #7D1439;
            font-size: 1.8em;
            margin-bottom: 20px;
            border-left: 4px solid #8F193E;
            padding-left: 15px;
        }
        .content-section h3 {
            color: #8F193E;
            font-size: 1.4em;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        .content-section h4 {
            color: #B01C3C;
            font-size: 1.2em;
            margin-top: 20px;
        }
        .content-section p {
            color: #555;
            line-height: 1.8;
            margin-bottom: 15px;
        }
        .concept-box {
            background: rgba(176, 28, 60, 0.08);
            border-left: 4px solid #B01C3C;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        .concept-box h3 {
            color: #B01C3C;
            margin-top: 0;
        }
        .concept-box.stage1 {
            background: rgba(143, 25, 62, 0.08);
            border-left-color: #8F193E;
        }
        .concept-box.stage1 h3 {
            color: #8F193E;
        }
        .concept-box.stage2 {
            background: rgba(125, 20, 57, 0.08);
            border-left-color: #7D1439;
        }
        .concept-box.stage2 h3 {
            color: #7D1439;
        }
        .highlight-box {
            background: linear-gradient(135deg, #8F193E 0%, #B01C3C 100%);
            color: white;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
        }
        .highlight-box strong {
            color: white;
        }
        .pipeline {
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 40px 0;
            flex-wrap: wrap;
            gap: 20px;
        }
        .stage-box {
            background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
            border-radius: 15px;
            padding: 30px;
            box-shadow: 0 4px 15px rgba(125, 20, 57, 0.08);
            text-align: center;
            min-width: 280px;
            flex: 1;
            border-top: 4px solid;
        }
        .stage-box.stage1 {
            border-top-color: #8F193E;
        }
        .stage-box.stage2 {
            border-top-color: #7D1439;
        }
        .stage-box h4 {
            color: #B01C3C;
            margin-top: 0;
            font-size: 1.3em;
        }
        .stage-arrow {
            font-size: 2.5em;
            color: #8F193E;
        }
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        .stat-card {
            background: linear-gradient(135deg, #8F193E 0%, #B01C3C 100%);
            color: white;
            padding: 25px;
            border-radius: 15px;
            text-align: center;
            box-shadow: 0 4px 15px rgba(125, 20, 57, 0.2);
            transition: all 0.3s ease;
        }
        .stat-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 25px rgba(143, 25, 62, 0.3);
        }
        .stat-card .number {
            font-size: 2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .stat-card .label {
            font-size: 0.9em;
            opacity: 0.95;
        }
        .training-phase {
            background: linear-gradient(135deg, #ffffff 0%, #f8f9fa 100%);
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            border-left: 4px solid #8F193E;
            box-shadow: 0 2px 10px rgba(125, 20, 57, 0.05);
        }
        .training-phase h4 {
            color: #8F193E;
            margin-top: 0;
        }
        .loss-formula {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 10px;
            font-family: 'Courier New', monospace;
            margin: 20px 0;
            text-align: center;
            font-size: 1.1em;
            box-shadow: 0 2px 10px rgba(0,0,0,0.2);
        }
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 30px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(125, 20, 57, 0.1);
        }
        .comparison-table th {
            background: linear-gradient(135deg, #8F193E 0%, #7D1439 100%);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: bold;
        }
        .comparison-table td {
            padding: 15px;
            border-bottom: 1px solid #f0f0f0;
            color: #555;
        }
        .comparison-table tr:last-child td {
            border-bottom: none;
        }
        .comparison-table tr:hover {
            background: rgba(143, 25, 62, 0.05);
        }
        .comparison-table tr.highlight {
            background: rgba(176, 28, 60, 0.1);
        }
        .tag {
            display: inline-block;
            color: white;
            padding: 5px 12px;
            border-radius: 15px;
            font-size: 0.85em;
            margin: 5px 5px 5px 0;
        }
        .tag.unsupervised {
            background: #8F193E;
        }
        .tag.weakly-supervised {
            background: #B01C3C;
        }
        .tag.self-supervised {
            background: #7D1439;
        }
        .cta-section {
            background: white;
            padding: 30px;
            border-radius: 15px;
            margin: 40px 0;
            text-align: center;
            box-shadow: 0 4px 15px rgba(125, 20, 57, 0.15);
        }
        .cta-section h3 {
            color: #7D1439;
            margin-bottom: 15px;
        }
        .cta-button {
            display: inline-block;
            color: white;
            padding: 12px 30px;
            border-radius: 25px;
            text-decoration: none;
            font-weight: bold;
            transition: all 0.3s ease;
            margin: 0 10px;
            background: linear-gradient(135deg, #8F193E 0%, #B01C3C 100%);
        }
        .cta-button:hover {
            background: linear-gradient(135deg, #7D1439 0%, #8F193E 100%);
            transform: translateY(-2px);
            box-shadow: 0 4px 10px rgba(143, 25, 62, 0.4);
        }
        footer a {
            color: #B01C3C;
            text-decoration: none;
            font-weight: bold;
        }
        footer a:hover {
            color: #7D1439;
        }
        @media (max-width: 768px) {
            .header {
                padding: 25px;
            }
            .header h1 {
                font-size: 2em;
            }
            .content-section {
                padding: 25px;
            }
            .pipeline {
                flex-direction: column;
            }
            .stage-arrow {
                transform: rotate(90deg);
            }
        }
    </style>
</head>
<body>
    <nav class="navbar">
        <div class="container">
            <ul class="navbar-nav">
                <li><a href="../index.html" class="home-link"><i class="fas fa-home"></i> Back to Learning Hub</a></li>
                <li><a href="#concept">Concept</a></li>
                <li><a href="#stages">Stages</a></li>
                <li><a href="#comparison">Comparison</a></li>
            </ul>
        </div>
    </nav>

    <main role="main">
        <div class="container">
            <div class="header">
                <h1><i class="fas fa-layer-group"></i> Two-Stage Foundation Model Training</h1>
                <p class="subtitle">Why modern foundation models separate unsupervised pretraining from supervised fine-tuning</p>
                <div class="paper-ref">
                    <strong>📊 Case Study:</strong> SATURN uses autoencoder pretraining + metric learning fine-tuning to achieve 119% improvement over end-to-end training baselines
                </div>
            </div>

            <!-- Core Concept -->
            <div class="content-section" id="concept">
                <h2><i class="fas fa-lightbulb"></i> Core Concept</h2>
                
                <div class="concept-box">
                    <h3>What is Two-Stage Training?</h3>
                    <p><strong>Two-stage training</strong> splits model development into distinct phases with different objectives:</p>
                    <ol>
                        <li><strong>Stage 1 (Pretraining):</strong> Learn general representations from large amounts of <span class="tag unsupervised">unsupervised</span> or <span class="tag self-supervised">self-supervised</span> data</li>
                        <li><strong>Stage 2 (Fine-tuning):</strong> Adapt representations to specific tasks using <span class="tag weakly-supervised">weakly supervised</span> or supervised signals</li>
                    </ol>
                </div>

                <h3>The Pipeline</h3>
                <div class="pipeline">
                    <div class="stage-box stage1">
                        <h4>Stage 1: Pretraining</h4>
                        <p><strong>Objective:</strong> Learn data structure</p>
                        <p><strong>Supervision:</strong> Self/Unsupervised</p>
                        <p><strong>Data:</strong> All available</p>
                    </div>
                    
                    <div class="stage-arrow">→</div>
                    
                    <div class="stage-box stage2">
                        <h4>Stage 2: Fine-tuning</h4>
                        <p><strong>Objective:</strong> Task-specific optimization</p>
                        <p><strong>Supervision:</strong> Labeled/Weak labels</p>
                        <p><strong>Data:</strong> Task-relevant subset</p>
                    </div>
                </div>

                <div class="highlight-box">
                    <strong><i class="fas fa-star"></i> Key Insight:</strong> 
                    Separating these stages allows the model to first learn universal data representations (Stage 1) before specializing for specific tasks (Stage 2). This is more effective than trying to learn both simultaneously.
                </div>
            </div>

            <!-- Stage 1 Deep Dive -->
            <div class="content-section" id="stages">
                <h2><i class="fas fa-brain"></i> Stage 1: Unsupervised Pretraining</h2>
                
                <div class="concept-box stage1">
                    <h3>Goals of Pretraining</h3>
                    <ul>
                        <li><strong>Learn data structure:</strong> Discover patterns, clusters, and relationships in raw data</li>
                        <li><strong>Initialize weights:</strong> Start Stage 2 from a good point, not random initialization</li>
                        <li><strong>Capture domain knowledge:</strong> Learn biological priors without task-specific labels</li>
                        <li><strong>Compression:</strong> Create low-dimensional representations that preserve information</li>
                    </ul>
                </div>

                <h3>Common Pretraining Objectives</h3>
                
                <div class="training-phase">
                    <h4>1. Autoencoder Reconstruction</h4>
                    <p><strong>Idea:</strong> Force model to reconstruct input from compressed representation</p>
                    <div class="loss-formula">
                        L<sub>pretrain</sub> = L<sub>reconstruction</sub>(X, Decoder(Encoder(X)))
                    </div>
                    <p><strong>Used by:</strong> SATURN (ZINB autoencoder), VAE-based models, β-VAE</p>
                    <p><strong>Why it works:</strong> To reconstruct accurately, encoder must capture important features</p>
                </div>

                <div class="training-phase">
                    <h4>2. Contrastive Learning</h4>
                    <p><strong>Idea:</strong> Similar samples should have similar embeddings, dissimilar samples should be far apart</p>
                    <div class="loss-formula">
                        L<sub>pretrain</sub> = -log( exp(sim(z<sub>i</sub>, z<sub>j</sub>)/τ) / Σ<sub>k</sub> exp(sim(z<sub>i</sub>, z<sub>k</sub>)/τ) )
                    </div>
                    <p><strong>Used by:</strong> SimCLR, MoCo, CLIP</p>
                </div>

                <div class="training-phase">
                    <h4>3. Masked Prediction</h4>
                    <p><strong>Idea:</strong> Predict masked parts of input from visible context</p>
                    <div class="loss-formula">
                        L<sub>pretrain</sub> = CrossEntropy(X<sub>masked</sub>, Predict(X<sub>visible</sub>))
                    </div>
                    <p><strong>Used by:</strong> BERT, GPT, ESM2 (protein language models)</p>
                </div>

                <h3>SATURN Stage 1 Example</h3>
                <div class="concept-box">
                    <p><strong>Objective:</strong> Learn universal cell embeddings across species</p>
                    <p><strong>Architecture:</strong> ZINB Autoencoder (appropriate for sparse scRNA-seq count data)</p>
                    <p><strong>Regularization:</strong> Gene-to-macrogene weights must reflect protein embedding similarity</p>
                    <p><strong>Result:</strong> Embeddings capture cell state + gene functional relationships</p>
                </div>

                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="number">50</div>
                        <div class="label">Pretraining epochs</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">335K</div>
                        <div class="label">Training cells</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">ZINB</div>
                        <div class="label">Loss function</div>
                    </div>
                </div>
            </div>

            <!-- Stage 2 Deep Dive -->
            <div class="content-section">
                <h2><i class="fas fa-crosshairs"></i> Stage 2: Task-Specific Fine-tuning</h2>
                
                <div class="concept-box stage2">
                    <h3>Goals of Fine-tuning</h3>
                    <ul>
                        <li><strong>Task adaptation:</strong> Specialize general representations for specific downstream task</li>
                        <li><strong>Incorporate labels:</strong> Use supervised/weakly supervised signals to improve performance</li>
                        <li><strong>Calibrate distances:</strong> Adjust embedding space so similar items are close</li>
                        <li><strong>Preserve pretraining:</strong> Don't catastrophically forget what was learned in Stage 1</li>
                    </ul>
                </div>

                <h3>Common Fine-tuning Objectives</h3>
                
                <div class="training-phase">
                    <h4>1. Supervised Classification</h4>
                    <p><strong>Idea:</strong> Add classification head on top of pretrained encoder, train with labeled data</p>
                    <div class="loss-formula">
                        L<sub>finetune</sub> = CrossEntropy(y, Classifier(Encoder(X)))
                    </div>
                    <p><strong>Used by:</strong> Most transfer learning scenarios</p>
                </div>

                <div class="training-phase">
                    <h4>2. Metric Learning</h4>
                    <p><strong>Idea:</strong> Learn distance metrics so embeddings reflect task-relevant similarity</p>
                    <div class="loss-formula">
                        L<sub>finetune</sub> = max(D(z<sub>anchor</sub>, z<sub>pos</sub>) - D(z<sub>anchor</sub>, z<sub>neg</sub>) + m, 0)
                    </div>
                    <p><strong>Used by:</strong> SATURN (triplet loss), face recognition, few-shot learning</p>
                </div>

                <h3>SATURN Stage 2 Example</h3>
                <div class="concept-box">
                    <p><strong>Objective:</strong> Align cell types across species</p>
                    <p><strong>Architecture:</strong> Triplet margin loss with semi-hard mining</p>
                    <p><strong>Supervision:</strong> Within-species cell type labels only</p>
                    <p><strong>Result:</strong> Cells of same type cluster together across species</p>
                </div>

                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="number">30</div>
                        <div class="label">Fine-tuning epochs</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">Triplet</div>
                        <div class="label">Loss function</div>
                    </div>
                    <div class="stat-card">
                        <div class="number">0.2</div>
                        <div class="label">Margin parameter</div>
                    </div>
                </div>
            </div>

            <!-- Comparison -->
            <div class="content-section" id="comparison">
                <h2><i class="fas fa-question-circle"></i> Why Two Stages Beat End-to-End Training</h2>

                <h3>Empirical Evidence from SATURN</h3>
                <table class="comparison-table">
                    <thead>
                        <tr>
                            <th>Training Strategy</th>
                            <th>Pretraining</th>
                            <th>Fine-tuning</th>
                            <th>Accuracy</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr class="highlight">
                            <td><strong>SATURN (Two-Stage)</strong></td>
                            <td>✅ ZINB Autoencoder</td>
                            <td>✅ Metric Learning</td>
                            <td><strong>85.8%</strong></td>
                        </tr>
                        <tr>
                            <td>SAMap (Two-Stage)</td>
                            <td>✅ Graph Neural Net</td>
                            <td>✅ Supervised</td>
                            <td>39.2%</td>
                        </tr>
                        <tr>
                            <td>Harmony (Single-Stage)</td>
                            <td>❌ None</td>
                            <td>❌ Direct</td>
                            <td>32.5%</td>
                        </tr>
                        <tr>
                            <td>scVI (Single-Stage)</td>
                            <td>❌ None</td>
                            <td>❌ Direct VAE</td>
                            <td>30.1%</td>
                        </tr>
                    </tbody>
                </table>

                <div class="highlight-box">
                    <strong><i class="fas fa-chart-line"></i> Performance Gain:</strong> 
                    SATURN's two-stage approach achieves 119% improvement over the best single-stage baseline (32.5% → 85.8%). This demonstrates the power of separating representation learning from task-specific optimization.
                </div>

                <h3>When End-to-End Might Be Better</h3>
                <div class="concept-box">
                    <p><strong>Consider end-to-end when:</strong></p>
                    <ul>
                        <li><strong>Limited data:</strong> If you have <1000 samples, pretraining may overfit</li>
                        <li><strong>Simple tasks:</strong> Linear classification doesn't need pretraining</li>
                        <li><strong>Task-specific features:</strong> Task requires very different representations</li>
                        <li><strong>Computational constraints:</strong> Two-stage takes ~2x wall-clock time</li>
                    </ul>
                </div>
            </div>

            <!-- Applications -->
            <div class="content-section">
                <h2><i class="fas fa-flask"></i> Applications Across Domains</h2>

                <div class="training-phase">
                    <h4>Natural Language Processing</h4>
                    <p><strong>Stage 1:</strong> Masked language modeling (BERT, GPT)</p>
                    <p><strong>Stage 2:</strong> Task-specific fine-tuning (sentiment, QA, summarization)</p>
                </div>

                <div class="training-phase">
                    <h4>Computer Vision</h4>
                    <p><strong>Stage 1:</strong> Self-supervised contrastive learning (SimCLR, MoCo)</p>
                    <p><strong>Stage 2:</strong> Object detection, segmentation, classification</p>
                </div>

                <div class="training-phase">
                    <h4>Protein Structure</h4>
                    <p><strong>Stage 1:</strong> Masked amino acid prediction (ESM2)</p>
                    <p><strong>Stage 2:</strong> Structure prediction (AlphaFold), function prediction</p>
                </div>

                <div class="training-phase">
                    <h4>Single-Cell Genomics</h4>
                    <p><strong>Stage 1:</strong> Variational autoencoder (scVI, SATURN)</p>
                    <p><strong>Stage 2:</strong> Cell type classification, trajectory inference</p>
                </div>
            </div>

            <div class="cta-section">
                <h3>📚 Continue Learning</h3>
                <p>Explore more foundation model concepts and machine learning techniques</p>
                <a href="../index.html" class="cta-button">
                    <i class="fas fa-arrow-left"></i> Back to Learning Hub
                </a>
            </div>
        </div>
    </main>

    <footer>
        <div class="container" style="text-align: center; margin-top: 40px; color: #666; padding-bottom: 30px;">
            <p>Part of the <a href="../index.html">AI4Bio Learning Hub</a> by Xinru Qiu</p>
            <p style="font-size: 0.9em; margin-top: 10px;">
                <a href="https://github.com/xqiu625" style="color: #666; margin: 0 10px;"><i class="fab fa-github"></i> GitHub</a> | 
                <a href="https://www.linkedin.com/in/xinru-qiu" style="color: #666; margin: 0 10px;"><i class="fab fa-linkedin"></i> LinkedIn</a> | 
                <a href="mailto:xinru.reina.qiu@gmail.com" style="color: #666; margin: 0 10px;"><i class="fas fa-envelope"></i> Email</a>
            </p>
        </div>
    </footer>
</body>
</html>
